{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33339558-4125-4fc3-b11f-a0ae3fcb0bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kshitijmac/de_on_gcp/Data_Engineering_GCP/gcp_venv/lib/python3.9/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gzip\n",
    "import dask.dataframe as dd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4ca58-fc55-4a36-9d4e-bde930bb2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# Path to the NYSE data\n",
    "path = '/Users/kshitijmac/Documents/Data_Files/data/nyse_all/nyse_data'\n",
    "\n",
    "# Function to measure execution time\n",
    "def measure_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        print(f\"{func.__name__} took {end_time - start_time:.2f} seconds to execute\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# -------------------- PANDAS --------------------\n",
    "@measure_time\n",
    "def process_with_pandas():\n",
    "    print(\"\\n===== Processing with Pandas =====\")\n",
    "    \n",
    "    # List all gzipped txt files in the directory\n",
    "    all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.txt.gz')]\n",
    "    \n",
    "    # Read the first file to explore structure\n",
    "    print(f\"Reading sample from {os.path.basename(all_files[0])}...\")\n",
    "    with gzip.open(all_files[0], 'rt') as f:\n",
    "        # Read first few lines to determine structure\n",
    "        lines = [next(f) for _ in range(6)]\n",
    "        \n",
    "    # Try to identify delimiter and structure\n",
    "    sample_line = lines[1]  # Skip header if exists\n",
    "    print(\"Sample line:\")\n",
    "    print(sample_line)\n",
    "    \n",
    "    # Assuming tab or comma-separated values, read the first file\n",
    "    sample_df = pd.read_csv(all_files[0], compression='gzip', sep=None, engine='python', nrows=5)\n",
    "    print(\"Sample data structure:\")\n",
    "    print(sample_df.head())\n",
    "    print(f\"Columns: {sample_df.columns.tolist()}\")\n",
    "    print(f\"Data types: {sample_df.dtypes}\")\n",
    "    \n",
    "    # Read all files into a single DataFrame\n",
    "    print(f\"Reading {len(all_files)} gzipped text files...\")\n",
    "    df_list = []\n",
    "    for file in all_files[:3]:  # Limiting to 3 files for demonstration\n",
    "        print(f\"Reading {os.path.basename(file)}...\")\n",
    "        df = pd.read_csv(file, compression='gzip')\n",
    "        df_list.append(df)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"Combined DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nBasic statistics:\")\n",
    "    if 'close' in df.columns:\n",
    "        print(f\"Average close price: {df['close'].mean():.2f}\")\n",
    "        print(f\"Max close price: {df['close'].max():.2f}\")\n",
    "        print(f\"Min close price: {df['close'].min():.2f}\")\n",
    "    elif 'Close' in df.columns:  # Check for capitalized column name\n",
    "        print(f\"Average close price: {df['Close'].mean():.2f}\")\n",
    "        print(f\"Max close price: {df['Close'].max():.2f}\")\n",
    "        print(f\"Min close price: {df['Close'].min():.2f}\")\n",
    "    \n",
    "    # Group by operation\n",
    "    symbol_col = None\n",
    "    price_col = None\n",
    "    \n",
    "    if 'symbol' in df.columns and 'close' in df.columns:\n",
    "        symbol_col, price_col = 'symbol', 'close'\n",
    "    elif 'Symbol' in df.columns and 'Close' in df.columns:\n",
    "        symbol_col, price_col = 'Symbol', 'Close'\n",
    "    \n",
    "    if symbol_col and price_col:\n",
    "        symbol_stats = df.groupby(symbol_col)[price_col].agg(['mean', 'min', 'max'])\n",
    "        print(\"\\nStats by symbol (top 5):\")\n",
    "        print(symbol_stats.head())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# -------------------- DASK --------------------\n",
    "@measure_time\n",
    "def process_with_dask():\n",
    "    print(\"\\n===== Processing with Dask =====\")\n",
    "    \n",
    "    # Create Dask DataFrame from gzipped text files\n",
    "    # Use glob pattern to match all gzipped files\n",
    "    dask_df = dd.read_csv(os.path.join(path, '*.txt.gz'), compression='gzip')\n",
    "    \n",
    "    print(\"Dask DataFrame information:\")\n",
    "    print(f\"Columns: {dask_df.columns.tolist()}\")\n",
    "    print(f\"Data types: {dask_df.dtypes}\")\n",
    "    \n",
    "    # Compute basic statistics (lazy evaluation until compute())\n",
    "    price_col = 'close' if 'close' in dask_df.columns else ('Close' if 'Close' in dask_df.columns else None)\n",
    "    \n",
    "    if price_col:\n",
    "        mean_close = dask_df[price_col].mean().compute()\n",
    "        max_close = dask_df[price_col].max().compute()\n",
    "        min_close = dask_df[price_col].min().compute()\n",
    "        \n",
    "        print(\"\\nBasic statistics:\")\n",
    "        print(f\"Average close price: {mean_close:.2f}\")\n",
    "        print(f\"Max close price: {max_close:.2f}\")\n",
    "        print(f\"Min close price: {min_close:.2f}\")\n",
    "    \n",
    "    # Group by operation\n",
    "    symbol_col = 'symbol' if 'symbol' in dask_df.columns else ('Symbol' if 'Symbol' in dask_df.columns else None)\n",
    "    \n",
    "    if symbol_col and price_col:\n",
    "        symbol_stats = dask_df.groupby(symbol_col)[price_col].agg(['mean', 'min', 'max']).compute()\n",
    "        print(\"\\nStats by symbol (top 5):\")\n",
    "        print(symbol_stats.head())\n",
    "    \n",
    "    return dask_df\n",
    "\n",
    "# -------------------- PYSPARK --------------------\n",
    "@measure_time\n",
    "@measure_time\n",
    "def process_with_pyspark():\n",
    "    print(\"\\n===== Processing with PySpark =====\")\n",
    "    \n",
    "    # Set Java environment variables before initializing Spark\n",
    "    import os\n",
    "    os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home'\n",
    "    os.environ['PATH'] = f\"{os.environ['JAVA_HOME']}/bin:{os.environ['PATH']}\"\n",
    "    \n",
    "    # Initialize SparkSession with additional configuration\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"NYSE_Data_Analysis\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Define schema - more robust version that handles different column name cases\n",
    "    schema = StructType([\n",
    "        StructField(\"date\", StringType(), True),\n",
    "        StructField(\"symbol\", StringType(), True),\n",
    "        StructField(\"open\", DoubleType(), True),\n",
    "        StructField(\"high\", DoubleType(), True),\n",
    "        StructField(\"low\", DoubleType(), True),\n",
    "        StructField(\"close\", DoubleType(), True),\n",
    "        StructField(\"volume\", DoubleType(), True)\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Read files with error handling\n",
    "        spark_df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .option(\"inferSchema\", \"true\") \\\n",
    "            .option(\"delimiter\", \"\\t\") \\\n",
    "            .option(\"nullValue\", \"null\") \\\n",
    "            .csv(os.path.join(path, \"*.txt.gz\"))\n",
    "        \n",
    "        # Standardize column names to lowercase\n",
    "        for col in spark_df.columns:\n",
    "            spark_df = spark_df.withColumnRenamed(col, col.lower())\n",
    "        \n",
    "        print(\"Spark DataFrame information:\")\n",
    "        spark_df.printSchema()\n",
    "        \n",
    "        # Show record count with error handling\n",
    "        try:\n",
    "            print(f\"Count: {spark_df.count():,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error counting records: {str(e)}\")\n",
    "        \n",
    "        print(\"Sample data:\")\n",
    "        spark_df.show(5, truncate=False)\n",
    "        \n",
    "        # Handle statistics - more robust column detection\n",
    "        price_col = 'close' if 'close' in spark_df.columns else None\n",
    "        symbol_col = 'symbol' if 'symbol' in spark_df.columns else None\n",
    "        \n",
    "        if price_col:\n",
    "            print(\"\\nBasic statistics:\")\n",
    "            spark_df.select(price_col).summary().show()\n",
    "        \n",
    "        if symbol_col and price_col:\n",
    "            print(f\"\\nStats by {symbol_col} (top 5):\")\n",
    "            from pyspark.sql.functions import mean, min, max\n",
    "            symbol_stats = spark_df.groupBy(symbol_col) \\\n",
    "                .agg(\n",
    "                    mean(price_col).alias(\"mean\"),\n",
    "                    min(price_col).alias(\"min\"),\n",
    "                    max(price_col).alias(\"max\")\n",
    "                ) \\\n",
    "                .orderBy(\"mean\", ascending=False)\n",
    "            symbol_stats.show(5, truncate=False)\n",
    "        \n",
    "        return spark_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        spark.stop()\n",
    "        print(\"Spark session stopped\")\n",
    "\n",
    "# Function to explore file structure\n",
    "def explore_file_structure():\n",
    "    print(\"\\n===== EXPLORING FILE STRUCTURE =====\")\n",
    "    \n",
    "    # List all gzipped txt files in the directory\n",
    "    all_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.txt.gz')]\n",
    "    if not all_files:\n",
    "        print(\"No .txt.gz files found in the specified directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(all_files)} .txt.gz files spanning from 1997 to 2017\")\n",
    "    \n",
    "    # Sample from first and last file to see if format changed over time\n",
    "    first_file = min(all_files)\n",
    "    last_file = max(all_files)\n",
    "    \n",
    "    print(f\"\\nExamining first file: {os.path.basename(first_file)}\")\n",
    "    with gzip.open(first_file, 'rt') as f:\n",
    "        first_lines = [next(f) for _ in range(3) if f]\n",
    "        print(\"First 3 lines:\")\n",
    "        for line in first_lines:\n",
    "            print(f\"  {line.strip()}\")\n",
    "    \n",
    "    print(f\"\\nExamining last file: {os.path.basename(last_file)}\")\n",
    "    with gzip.open(last_file, 'rt') as f:\n",
    "        last_lines = [next(f) for _ in range(3) if f]\n",
    "        print(\"First 3 lines:\")\n",
    "        for line in last_lines:\n",
    "            print(f\"  {line.strip()}\")\n",
    "    \n",
    "    # Get file sizes\n",
    "    sizes = [os.path.getsize(f) for f in all_files]\n",
    "    total_size_gb = sum(sizes) / (1024**3)\n",
    "    \n",
    "    print(f\"\\nTotal dataset size: {total_size_gb:.2f} GB\")\n",
    "    print(f\"Average file size: {sum(sizes)/len(sizes)/1024**2:.2f} MB\")\n",
    "    print(f\"Largest file: {max(sizes)/1024**2:.2f} MB\")\n",
    "    print(f\"Smallest file: {min(sizes)/1024**2:.2f} MB\")\n",
    "\n",
    "# -------------------- COMPARISON --------------------\n",
    "def compare_frameworks():\n",
    "    print(\"\\n===== FRAMEWORK COMPARISON =====\")\n",
    "    \n",
    "    print(\"1. Pandas:\")\n",
    "    print(\"   - In-memory processing (requires all data to fit in RAM)\")\n",
    "    print(\"   - Easiest to use and most intuitive API\")\n",
    "    print(\"   - Best for datasets up to a few GB\")\n",
    "    print(\"   - Single-threaded by default\")\n",
    "    print(\"   - Good for gzipped files but processes them sequentially\")\n",
    "    \n",
    "    print(\"\\n2. Dask:\")\n",
    "    print(\"   - Extends Pandas API for larger-than-memory datasets\")\n",
    "    print(\"   - Parallel processing on a single machine\")\n",
    "    print(\"   - Lazy evaluation (operations execute only when results are needed)\")\n",
    "    print(\"   - Good middle ground for medium-sized datasets (10s-100s of GB)\")\n",
    "    print(\"   - Can efficiently handle multiple gzipped files in parallel\")\n",
    "    \n",
    "    print(\"\\n3. PySpark:\")\n",
    "    print(\"   - Distributed processing across clusters\")\n",
    "    print(\"   - Most complex setup but most scalable\")\n",
    "    print(\"   - Best for very large datasets (100s of GB to TB+)\")\n",
    "    print(\"   - Built-in fault tolerance\")\n",
    "    print(\"   - Requires more boilerplate code\")\n",
    "    print(\"   - Automatic handling of compression formats\")\n",
    "    print(\"   - Best for processing all 21 years of NYSE data efficiently\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e2c563-72eb-49c9-b0e2-d50fd8635b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== EXPLORING FILE STRUCTURE =====\n",
      "Found 21 .txt.gz files spanning from 1997 to 2017\n",
      "\n",
      "Examining first file: NYSE_1997.txt.gz\n",
      "First 3 lines:\n",
      "  AA,19970101,47.82,47.82,47.82,47.82,0\n",
      "  ABC,19970101,6.03,6.03,6.03,6.03,0\n",
      "  ABM,19970101,9.25,9.25,9.25,9.25,0\n",
      "\n",
      "Examining last file: NYSE_2017.txt.gz\n",
      "First 3 lines:\n",
      "  A,20170102,45.56,45.56,45.56,45.56,0\n",
      "  AA,20170102,28.08,28.08,28.08,28.08,0\n",
      "  AAC,20170102,7.24,7.24,7.24,7.24,0\n",
      "\n",
      "Total dataset size: 0.13 GB\n",
      "Average file size: 6.32 MB\n",
      "Largest file: 11.25 MB\n",
      "Smallest file: 0.50 MB\n",
      "\n",
      "Processing with each framework (limited to 3 files for demonstration):\n",
      "\n",
      "===== Processing with Pandas =====\n",
      "Reading sample from NYSE_2007.txt.gz...\n",
      "Sample line:\n",
      "AA,20070101,90.03,90.03,90.03,90.03,0\n",
      "\n",
      "Sample data structure:\n",
      "     A  20070101  34.85  34.85.1  34.85.2  34.85.3  0\n",
      "0   AA  20070101  90.03    90.03    90.03    90.03  0\n",
      "1  AAP  20070101  35.56    35.56    35.56    35.56  0\n",
      "2  AAV  20070101  10.68    10.68    10.68    10.68  0\n",
      "3   AB  20070101  80.40    80.40    80.40    80.40  0\n",
      "4  ABB  20070101  17.98    17.98    17.98    17.98  0\n",
      "Columns: ['A', '20070101', '34.85', '34.85.1', '34.85.2', '34.85.3', '0']\n",
      "Data types: A            object\n",
      "20070101      int64\n",
      "34.85       float64\n",
      "34.85.1     float64\n",
      "34.85.2     float64\n",
      "34.85.3     float64\n",
      "0             int64\n",
      "dtype: object\n",
      "Reading 21 gzipped text files...\n",
      "Reading NYSE_2007.txt.gz...\n",
      "Reading NYSE_2015.txt.gz...\n",
      "Reading NYSE_2017.txt.gz...\n",
      "Combined DataFrame shape: (1249543, 17)\n",
      "\n",
      "Basic statistics:\n",
      "process_with_pandas took 0.39 seconds to execute\n",
      "\n",
      "===== Processing with Dask =====\n",
      "Dask DataFrame information:\n",
      "Columns: ['AA', '19970101', '47.82', '47.82.1', '47.82.2', '47.82.3', '0']\n",
      "Data types: AA           object\n",
      "19970101      int64\n",
      "47.82       float64\n",
      "47.82.1     float64\n",
      "47.82.2     float64\n",
      "47.82.3     float64\n",
      "0             int64\n",
      "dtype: object\n",
      "process_with_dask took 0.01 seconds to execute\n",
      "\n",
      "===== Processing with PySpark =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kshitijmac/de_on_gcp/Data_Engineering_GCP/gcp_venv/lib/python3.9/site-packages/dask/dataframe/io/csv.py:555: UserWarning: Warning gzip compression does not support breaking apart files\n",
      "Please ensure that each individual file can fit in memory and\n",
      "use the keyword ``blocksize=None to remove this message``\n",
      "Setting ``blocksize=None``\n",
      "  warn(\n",
      "25/05/09 08:10:07 WARN Utils: Your hostname, Manasi-ka-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.22 instead (on interface en0)\n",
      "25/05/09 08:10:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/09 08:10:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark DataFrame information:\n",
      "root\n",
      " |-- a,20160101,41.81,41.81,41.81,41.81,0: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 9,384,718\n",
      "Sample data:\n",
      "+------------------------------------------+\n",
      "|a,20160101,41.81,41.81,41.81,41.81,0      |\n",
      "+------------------------------------------+\n",
      "|AA,20160101,29.61,29.61,29.61,29.61,0     |\n",
      "|AAC,20160101,19.06,19.06,19.06,19.06,0    |\n",
      "|AAN,20160101,22.39,22.39,22.39,22.39,0    |\n",
      "|AAP,20160101,150.51,150.51,150.51,150.51,0|\n",
      "|AAT,20160101,38.35,38.35,38.35,38.35,0    |\n",
      "+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Spark session stopped\n",
      "process_with_pyspark took 14.85 seconds to execute\n",
      "wrapper took 14.85 seconds to execute\n",
      "\n",
      "===== FRAMEWORK COMPARISON =====\n",
      "1. Pandas:\n",
      "   - In-memory processing (requires all data to fit in RAM)\n",
      "   - Easiest to use and most intuitive API\n",
      "   - Best for datasets up to a few GB\n",
      "   - Single-threaded by default\n",
      "   - Good for gzipped files but processes them sequentially\n",
      "\n",
      "2. Dask:\n",
      "   - Extends Pandas API for larger-than-memory datasets\n",
      "   - Parallel processing on a single machine\n",
      "   - Lazy evaluation (operations execute only when results are needed)\n",
      "   - Good middle ground for medium-sized datasets (10s-100s of GB)\n",
      "   - Can efficiently handle multiple gzipped files in parallel\n",
      "\n",
      "3. PySpark:\n",
      "   - Distributed processing across clusters\n",
      "   - Most complex setup but most scalable\n",
      "   - Best for very large datasets (100s of GB to TB+)\n",
      "   - Built-in fault tolerance\n",
      "   - Requires more boilerplate code\n",
      "   - Automatic handling of compression formats\n",
      "   - Best for processing all 21 years of NYSE data efficiently\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "\n",
    "# Run all three approaches and compare\n",
    "if __name__ == \"__main__\":\n",
    "    # Set pandas display options\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    try:\n",
    "        # First explore the file structure\n",
    "        explore_file_structure()\n",
    "        \n",
    "        # Run each framework\n",
    "        print(\"\\nProcessing with each framework (limited to 3 files for demonstration):\")\n",
    "        pandas_df = process_with_pandas()\n",
    "        dask_df = process_with_dask()\n",
    "        spark_df = process_with_pyspark()\n",
    "        \n",
    "        # Show comparison of approaches\n",
    "        compare_frameworks()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
